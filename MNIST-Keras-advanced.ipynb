{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# MNIST- handwritten digit recognition - Advanced Topics\n",
    "In this modeule, we will talk about how we can further improve performance using various techniques.\n",
    "\n",
    "## Batch Normalization\n",
    "Do you remember we normalized input images such that they have zero mean? The training converges faster when images are normalized (zero mean and unit variance) and decorrelated. However, the parameter update during the training changes distributions in each layer, which is called *internal covariant shift*. Ioffe and Szegedy suggested [batch normalization](https://arxiv.org/abs/1502.03167) to normalize and decorrelate inputs to the mid-layers to resolve this issue and make the netwrok training converges faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "9s - loss: 0.7538 - acc: 0.8396 - val_loss: 2.8911 - val_acc: 0.1145\n",
      "Epoch 2/20\n",
      "8s - loss: 0.2869 - acc: 0.9607 - val_loss: 0.1399 - val_acc: 0.9859\n",
      "Epoch 3/20\n",
      "8s - loss: 0.1850 - acc: 0.9717 - val_loss: 0.0849 - val_acc: 0.9880\n",
      "Epoch 4/20\n",
      "8s - loss: 0.1328 - acc: 0.9777 - val_loss: 0.0587 - val_acc: 0.9906\n",
      "Epoch 5/20\n",
      "8s - loss: 0.1044 - acc: 0.9808 - val_loss: 0.0442 - val_acc: 0.9919\n",
      "Epoch 6/20\n",
      "8s - loss: 0.0881 - acc: 0.9826 - val_loss: 0.0405 - val_acc: 0.9911\n",
      "Epoch 7/20\n",
      "8s - loss: 0.0754 - acc: 0.9837 - val_loss: 0.0379 - val_acc: 0.9910\n",
      "Epoch 8/20\n",
      "8s - loss: 0.0686 - acc: 0.9844 - val_loss: 0.0330 - val_acc: 0.9921\n",
      "Epoch 9/20\n",
      "8s - loss: 0.0615 - acc: 0.9859 - val_loss: 0.0250 - val_acc: 0.9937\n",
      "Epoch 10/20\n",
      "8s - loss: 0.0559 - acc: 0.9866 - val_loss: 0.0234 - val_acc: 0.9940\n",
      "Epoch 11/20\n",
      "8s - loss: 0.0521 - acc: 0.9868 - val_loss: 0.0227 - val_acc: 0.9940\n",
      "Epoch 12/20\n",
      "8s - loss: 0.0501 - acc: 0.9872 - val_loss: 0.0227 - val_acc: 0.9945\n",
      "Epoch 13/20\n",
      "8s - loss: 0.0454 - acc: 0.9882 - val_loss: 0.0184 - val_acc: 0.9951\n",
      "Epoch 14/20\n",
      "8s - loss: 0.0432 - acc: 0.9886 - val_loss: 0.0210 - val_acc: 0.9940\n",
      "Epoch 15/20\n",
      "8s - loss: 0.0421 - acc: 0.9888 - val_loss: 0.0214 - val_acc: 0.9942\n",
      "Epoch 16/20\n",
      "8s - loss: 0.0407 - acc: 0.9886 - val_loss: 0.0156 - val_acc: 0.9959\n",
      "Epoch 17/20\n",
      "8s - loss: 0.0357 - acc: 0.9904 - val_loss: 0.0170 - val_acc: 0.9956\n",
      "Epoch 18/20\n",
      "8s - loss: 0.0356 - acc: 0.9903 - val_loss: 0.0190 - val_acc: 0.9949\n",
      "Epoch 19/20\n",
      "8s - loss: 0.0367 - acc: 0.9896 - val_loss: 0.0178 - val_acc: 0.9948\n",
      "Epoch 20/20\n",
      "8s - loss: 0.0354 - acc: 0.9901 - val_loss: 0.0137 - val_acc: 0.9966\n",
      "CNN Error: 0.34%\n"
     ]
    }
   ],
   "source": [
    "# Implement Batch Normalization\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering( 'tf' )\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][width][height][channel]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype( 'float32' )\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype( 'float32' )\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "def BN_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, 3, 3, input_shape=(28, 28, 1), border_mode='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(16, 3, 3, border_mode='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "    return model\n",
    "# build the model\n",
    "model = BN_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=20, batch_size=200,\n",
    "verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Controversy**: Some people say they got better result by reversing the order of Activation and BatchNorm, but in this case, reversing order gave worse result. In principle, the BatchNorm layer should come before the Activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "9s - loss: 8.0731 - acc: 0.2075 - val_loss: 10.1180 - val_acc: 0.1028\n",
      "Epoch 2/20\n",
      "8s - loss: 7.1452 - acc: 0.3332 - val_loss: 5.3596 - val_acc: 0.4771\n",
      "Epoch 3/20\n",
      "8s - loss: 6.6917 - acc: 0.4535 - val_loss: 5.1897 - val_acc: 0.6322\n",
      "Epoch 4/20\n",
      "7s - loss: 6.1551 - acc: 0.4118 - val_loss: 4.5167 - val_acc: 0.5369\n",
      "Epoch 5/20\n",
      "8s - loss: 5.4976 - acc: 0.5046 - val_loss: 3.3844 - val_acc: 0.6889\n",
      "Epoch 6/20\n",
      "7s - loss: 5.6653 - acc: 0.5611 - val_loss: 5.4644 - val_acc: 0.6904\n",
      "Epoch 7/20\n",
      "7s - loss: 5.7607 - acc: 0.5897 - val_loss: 5.8210 - val_acc: 0.6806\n",
      "Epoch 8/20\n",
      "8s - loss: 6.0808 - acc: 0.5940 - val_loss: 4.6271 - val_acc: 0.6558\n",
      "Epoch 9/20\n",
      "8s - loss: 7.3704 - acc: 0.5107 - val_loss: 6.8315 - val_acc: 0.5223\n",
      "Epoch 10/20\n",
      "8s - loss: 7.6610 - acc: 0.4419 - val_loss: 8.4770 - val_acc: 0.4536\n",
      "Epoch 11/20\n",
      "8s - loss: 7.5410 - acc: 0.3657 - val_loss: 6.6829 - val_acc: 0.3459\n",
      "Epoch 12/20\n",
      "8s - loss: 6.9014 - acc: 0.3016 - val_loss: 6.7791 - val_acc: 0.3182\n",
      "Epoch 13/20\n",
      "8s - loss: 6.1909 - acc: 0.4610 - val_loss: 4.1577 - val_acc: 0.5241\n",
      "Epoch 14/20\n",
      "8s - loss: 6.1496 - acc: 0.5490 - val_loss: 6.6538 - val_acc: 0.6681\n",
      "Epoch 15/20\n",
      "8s - loss: 7.1950 - acc: 0.6042 - val_loss: 8.8452 - val_acc: 0.6819\n",
      "Epoch 16/20\n",
      "8s - loss: 7.4705 - acc: 0.6193 - val_loss: 8.8157 - val_acc: 0.6944\n",
      "Epoch 17/20\n",
      "8s - loss: 7.3176 - acc: 0.6064 - val_loss: 7.9154 - val_acc: 0.6351\n",
      "Epoch 18/20\n",
      "8s - loss: 6.4013 - acc: 0.5344 - val_loss: 6.2010 - val_acc: 0.5369\n",
      "Epoch 19/20\n",
      "8s - loss: 5.6181 - acc: 0.4077 - val_loss: 5.1152 - val_acc: 0.4328\n",
      "Epoch 20/20\n",
      "8s - loss: 5.0619 - acc: 0.4214 - val_loss: 4.3131 - val_acc: 0.5153\n",
      "CNN Error: 48.47%\n"
     ]
    }
   ],
   "source": [
    "# Implement Batch Normalization - after the activation \n",
    "\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering( 'tf' )\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][width][height][channel]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype( 'float32' )\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype( 'float32' )\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "def BNr_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, 3, 3, input_shape=(28, 28, 1), border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Convolution2D(16, 3, 3, border_mode='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.add(BatchNormalization())\n",
    "    # Compile model\n",
    "    model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "    return model\n",
    "# build the model\n",
    "model = BNr_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=20, batch_size=200,\n",
    "verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geena/anaconda3/envs/DLK1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1917: UserWarning: Expected no kwargs, you passed 1\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "7s - loss: 0.3771 - acc: 0.9605 - val_loss: 1.5870 - val_acc: 0.3883\n",
      "Epoch 2/50\n",
      "7s - loss: 0.1642 - acc: 0.9900 - val_loss: 0.1376 - val_acc: 0.9910\n",
      "Epoch 3/50\n",
      "7s - loss: 0.1020 - acc: 0.9934 - val_loss: 0.1412 - val_acc: 0.9834\n",
      "Epoch 4/50\n",
      "7s - loss: 0.0701 - acc: 0.9954 - val_loss: 0.0660 - val_acc: 0.9930\n",
      "Epoch 5/50\n",
      "6s - loss: 0.0505 - acc: 0.9970 - val_loss: 0.0674 - val_acc: 0.9921\n",
      "Epoch 6/50\n",
      "7s - loss: 0.0385 - acc: 0.9972 - val_loss: 0.0526 - val_acc: 0.9922\n",
      "Epoch 7/50\n",
      "7s - loss: 0.0304 - acc: 0.9979 - val_loss: 0.0362 - val_acc: 0.9937\n",
      "Epoch 8/50\n",
      "7s - loss: 0.0242 - acc: 0.9984 - val_loss: 0.0410 - val_acc: 0.9921\n",
      "Epoch 9/50\n",
      "7s - loss: 0.0199 - acc: 0.9986 - val_loss: 0.0471 - val_acc: 0.9897\n",
      "Epoch 10/50\n",
      "7s - loss: 0.0165 - acc: 0.9986 - val_loss: 0.0469 - val_acc: 0.9893\n",
      "Epoch 11/50\n",
      "7s - loss: 0.0138 - acc: 0.9989 - val_loss: 0.0327 - val_acc: 0.9922\n",
      "Epoch 12/50\n",
      "7s - loss: 0.0109 - acc: 0.9992 - val_loss: 0.0288 - val_acc: 0.9919\n",
      "Epoch 13/50\n",
      "7s - loss: 0.0115 - acc: 0.9986 - val_loss: 0.0418 - val_acc: 0.9894\n",
      "Epoch 14/50\n",
      "7s - loss: 0.0101 - acc: 0.9987 - val_loss: 0.0381 - val_acc: 0.9905\n",
      "Epoch 15/50\n",
      "7s - loss: 0.0088 - acc: 0.9988 - val_loss: 0.0293 - val_acc: 0.9918\n",
      "Epoch 16/50\n",
      "7s - loss: 0.0078 - acc: 0.9990 - val_loss: 0.0253 - val_acc: 0.9932\n",
      "Epoch 17/50\n",
      "7s - loss: 0.0060 - acc: 0.9992 - val_loss: 0.0286 - val_acc: 0.9915\n",
      "Epoch 18/50\n",
      "7s - loss: 0.0079 - acc: 0.9987 - val_loss: 0.0233 - val_acc: 0.9933\n",
      "Epoch 19/50\n",
      "7s - loss: 0.0049 - acc: 0.9994 - val_loss: 0.0253 - val_acc: 0.9933\n",
      "Epoch 20/50\n",
      "7s - loss: 0.0057 - acc: 0.9990 - val_loss: 0.0206 - val_acc: 0.9947\n",
      "Epoch 21/50\n",
      "7s - loss: 0.0048 - acc: 0.9993 - val_loss: 0.0240 - val_acc: 0.9927\n",
      "Epoch 22/50\n",
      "7s - loss: 0.0055 - acc: 0.9990 - val_loss: 0.0280 - val_acc: 0.9920\n",
      "Epoch 23/50\n",
      "7s - loss: 0.0042 - acc: 0.9992 - val_loss: 0.0292 - val_acc: 0.9915\n",
      "Epoch 24/50\n",
      "7s - loss: 0.0035 - acc: 0.9994 - val_loss: 0.0282 - val_acc: 0.9920\n",
      "Epoch 25/50\n",
      "7s - loss: 0.0025 - acc: 0.9996 - val_loss: 0.0202 - val_acc: 0.9941\n",
      "Epoch 26/50\n",
      "7s - loss: 0.0021 - acc: 0.9997 - val_loss: 0.0256 - val_acc: 0.9926\n",
      "Epoch 27/50\n",
      "7s - loss: 0.0039 - acc: 0.9991 - val_loss: 0.0410 - val_acc: 0.9886\n",
      "Epoch 28/50\n",
      "7s - loss: 0.0046 - acc: 0.9988 - val_loss: 0.0232 - val_acc: 0.9931\n",
      "Epoch 29/50\n",
      "7s - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0393 - val_acc: 0.9889\n",
      "Epoch 30/50\n",
      "7s - loss: 0.0022 - acc: 0.9996 - val_loss: 0.0229 - val_acc: 0.9935\n",
      "Epoch 31/50\n",
      "7s - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0194 - val_acc: 0.9945\n",
      "Epoch 32/50\n",
      "7s - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0218 - val_acc: 0.9936\n",
      "Epoch 33/50\n",
      "7s - loss: 0.0015 - acc: 0.9998 - val_loss: 0.0242 - val_acc: 0.9936\n",
      "Epoch 34/50\n",
      "7s - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0295 - val_acc: 0.9922\n",
      "Epoch 35/50\n",
      "7s - loss: 0.0048 - acc: 0.9987 - val_loss: 0.0291 - val_acc: 0.9922\n",
      "Epoch 36/50\n",
      "7s - loss: 0.0019 - acc: 0.9995 - val_loss: 0.0269 - val_acc: 0.9914\n",
      "Epoch 37/50\n",
      "7s - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0284 - val_acc: 0.9920\n",
      "Epoch 38/50\n",
      "7s - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0260 - val_acc: 0.9928\n",
      "Epoch 39/50\n",
      "7s - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0227 - val_acc: 0.9936\n",
      "Epoch 40/50\n",
      "7s - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0219 - val_acc: 0.9942\n",
      "Epoch 41/50\n",
      "7s - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0287 - val_acc: 0.9922\n",
      "Epoch 42/50\n",
      "7s - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0302 - val_acc: 0.9916\n",
      "Epoch 43/50\n",
      "7s - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0266 - val_acc: 0.9933\n",
      "Epoch 44/50\n",
      "7s - loss: 7.5105e-04 - acc: 0.9998 - val_loss: 0.0225 - val_acc: 0.9940\n",
      "Epoch 45/50\n",
      "7s - loss: 4.4932e-04 - acc: 0.9999 - val_loss: 0.0277 - val_acc: 0.9925\n",
      "Epoch 46/50\n",
      "7s - loss: 0.0017 - acc: 0.9997 - val_loss: 0.0275 - val_acc: 0.9926\n",
      "Epoch 47/50\n",
      "7s - loss: 0.0016 - acc: 0.9996 - val_loss: 0.0340 - val_acc: 0.9908\n",
      "Epoch 48/50\n",
      "7s - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0242 - val_acc: 0.9933\n",
      "Epoch 49/50\n",
      "7s - loss: 7.2939e-04 - acc: 0.9999 - val_loss: 0.0220 - val_acc: 0.9935\n",
      "Epoch 50/50\n",
      "7s - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0241 - val_acc: 0.9938\n",
      "CNN Error: 0.62%\n"
     ]
    }
   ],
   "source": [
    "# Implement Batch Normalization\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.callbacks import History\n",
    "\n",
    "K.set_image_dim_ordering( 'tf' )\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][width][height][channel]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype( 'float32' )\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype( 'float32' )\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "def BN_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, 3, 3, input_shape=(28, 28, 1), border_mode='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(16, 3, 3, border_mode='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(64))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    # Compile model\n",
    "    adam = Adam(lr=0.001)\n",
    "    history = History()\n",
    "    model.compile(loss= 'categorical_crossentropy' , optimizer= adam , metrics=[ 'accuracy' ], callbacks=history)\n",
    "    return model\n",
    "# build the model\n",
    "model = BN_model()\n",
    "\n",
    "# Fit the model\n",
    "log = model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=50, batch_size=200,\n",
    "verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEgKKdxFRFLo/wO96/7eS5555AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fad28c7ad30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(log.history['acc']) \n",
    "plt.plot(log.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
