{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# MNIST- handwritten digit recognition - Advanced Topics - Part 1\n",
    "In this modeule, we will talk about how we can further improve performance using various techniques.\n",
    "\n",
    "## Batch Normalization\n",
    "Do you remember we normalized input images such that they have zero mean? The training converges faster when images are normalized (zero mean and unit variance) and decorrelated. However, the parameter update during the training changes distributions in each layer, which is called *internal covariant shift*. Ioffe and Szegedy suggested [batch normalization](https://arxiv.org/abs/1502.03167) to normalize and decorrelate inputs to the mid-layers to resolve this issue and make the netwrok training converges faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "9s - loss: 0.7538 - acc: 0.8396 - val_loss: 2.8911 - val_acc: 0.1145\n",
      "Epoch 2/20\n",
      "8s - loss: 0.2869 - acc: 0.9607 - val_loss: 0.1399 - val_acc: 0.9859\n",
      "Epoch 3/20\n",
      "8s - loss: 0.1850 - acc: 0.9717 - val_loss: 0.0849 - val_acc: 0.9880\n",
      "Epoch 4/20\n",
      "8s - loss: 0.1328 - acc: 0.9777 - val_loss: 0.0587 - val_acc: 0.9906\n",
      "Epoch 5/20\n",
      "8s - loss: 0.1044 - acc: 0.9808 - val_loss: 0.0442 - val_acc: 0.9919\n",
      "Epoch 6/20\n",
      "8s - loss: 0.0881 - acc: 0.9826 - val_loss: 0.0405 - val_acc: 0.9911\n",
      "Epoch 7/20\n",
      "8s - loss: 0.0754 - acc: 0.9837 - val_loss: 0.0379 - val_acc: 0.9910\n",
      "Epoch 8/20\n",
      "8s - loss: 0.0686 - acc: 0.9844 - val_loss: 0.0330 - val_acc: 0.9921\n",
      "Epoch 9/20\n",
      "8s - loss: 0.0615 - acc: 0.9859 - val_loss: 0.0250 - val_acc: 0.9937\n",
      "Epoch 10/20\n",
      "8s - loss: 0.0559 - acc: 0.9866 - val_loss: 0.0234 - val_acc: 0.9940\n",
      "Epoch 11/20\n",
      "8s - loss: 0.0521 - acc: 0.9868 - val_loss: 0.0227 - val_acc: 0.9940\n",
      "Epoch 12/20\n",
      "8s - loss: 0.0501 - acc: 0.9872 - val_loss: 0.0227 - val_acc: 0.9945\n",
      "Epoch 13/20\n",
      "8s - loss: 0.0454 - acc: 0.9882 - val_loss: 0.0184 - val_acc: 0.9951\n",
      "Epoch 14/20\n",
      "8s - loss: 0.0432 - acc: 0.9886 - val_loss: 0.0210 - val_acc: 0.9940\n",
      "Epoch 15/20\n",
      "8s - loss: 0.0421 - acc: 0.9888 - val_loss: 0.0214 - val_acc: 0.9942\n",
      "Epoch 16/20\n",
      "8s - loss: 0.0407 - acc: 0.9886 - val_loss: 0.0156 - val_acc: 0.9959\n",
      "Epoch 17/20\n",
      "8s - loss: 0.0357 - acc: 0.9904 - val_loss: 0.0170 - val_acc: 0.9956\n",
      "Epoch 18/20\n",
      "8s - loss: 0.0356 - acc: 0.9903 - val_loss: 0.0190 - val_acc: 0.9949\n",
      "Epoch 19/20\n",
      "8s - loss: 0.0367 - acc: 0.9896 - val_loss: 0.0178 - val_acc: 0.9948\n",
      "Epoch 20/20\n",
      "8s - loss: 0.0354 - acc: 0.9901 - val_loss: 0.0137 - val_acc: 0.9966\n",
      "CNN Error: 0.34%\n"
     ]
    }
   ],
   "source": [
    "# Implement Batch Normalization\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering( 'tf' )\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][width][height][channel]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype( 'float32' )\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype( 'float32' )\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "def BN_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, 3, 3, input_shape=(28, 28, 1), border_mode='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(16, 3, 3, border_mode='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "    return model\n",
    "# build the model\n",
    "model = BN_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=20, batch_size=200,\n",
    "verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Controversy**: Some people say they got better result by reversing the order of Activation and BatchNorm, but in this case, reversing order gave worse result. In principle, the BatchNorm layer should come before the Activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "9s - loss: 8.0731 - acc: 0.2075 - val_loss: 10.1180 - val_acc: 0.1028\n",
      "Epoch 2/20\n",
      "8s - loss: 7.1452 - acc: 0.3332 - val_loss: 5.3596 - val_acc: 0.4771\n",
      "Epoch 3/20\n",
      "8s - loss: 6.6917 - acc: 0.4535 - val_loss: 5.1897 - val_acc: 0.6322\n",
      "Epoch 4/20\n",
      "7s - loss: 6.1551 - acc: 0.4118 - val_loss: 4.5167 - val_acc: 0.5369\n",
      "Epoch 5/20\n",
      "8s - loss: 5.4976 - acc: 0.5046 - val_loss: 3.3844 - val_acc: 0.6889\n",
      "Epoch 6/20\n",
      "7s - loss: 5.6653 - acc: 0.5611 - val_loss: 5.4644 - val_acc: 0.6904\n",
      "Epoch 7/20\n",
      "7s - loss: 5.7607 - acc: 0.5897 - val_loss: 5.8210 - val_acc: 0.6806\n",
      "Epoch 8/20\n",
      "8s - loss: 6.0808 - acc: 0.5940 - val_loss: 4.6271 - val_acc: 0.6558\n",
      "Epoch 9/20\n",
      "8s - loss: 7.3704 - acc: 0.5107 - val_loss: 6.8315 - val_acc: 0.5223\n",
      "Epoch 10/20\n",
      "8s - loss: 7.6610 - acc: 0.4419 - val_loss: 8.4770 - val_acc: 0.4536\n",
      "Epoch 11/20\n",
      "8s - loss: 7.5410 - acc: 0.3657 - val_loss: 6.6829 - val_acc: 0.3459\n",
      "Epoch 12/20\n",
      "8s - loss: 6.9014 - acc: 0.3016 - val_loss: 6.7791 - val_acc: 0.3182\n",
      "Epoch 13/20\n",
      "8s - loss: 6.1909 - acc: 0.4610 - val_loss: 4.1577 - val_acc: 0.5241\n",
      "Epoch 14/20\n",
      "8s - loss: 6.1496 - acc: 0.5490 - val_loss: 6.6538 - val_acc: 0.6681\n",
      "Epoch 15/20\n",
      "8s - loss: 7.1950 - acc: 0.6042 - val_loss: 8.8452 - val_acc: 0.6819\n",
      "Epoch 16/20\n",
      "8s - loss: 7.4705 - acc: 0.6193 - val_loss: 8.8157 - val_acc: 0.6944\n",
      "Epoch 17/20\n",
      "8s - loss: 7.3176 - acc: 0.6064 - val_loss: 7.9154 - val_acc: 0.6351\n",
      "Epoch 18/20\n",
      "8s - loss: 6.4013 - acc: 0.5344 - val_loss: 6.2010 - val_acc: 0.5369\n",
      "Epoch 19/20\n",
      "8s - loss: 5.6181 - acc: 0.4077 - val_loss: 5.1152 - val_acc: 0.4328\n",
      "Epoch 20/20\n",
      "8s - loss: 5.0619 - acc: 0.4214 - val_loss: 4.3131 - val_acc: 0.5153\n",
      "CNN Error: 48.47%\n"
     ]
    }
   ],
   "source": [
    "# Implement Batch Normalization - after the activation \n",
    "\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering( 'tf' )\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][width][height][channel]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype( 'float32' )\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype( 'float32' )\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "def BNr_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, 3, 3, input_shape=(28, 28, 1), border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Convolution2D(16, 3, 3, border_mode='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.add(BatchNormalization())\n",
    "    # Compile model\n",
    "    model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "    return model\n",
    "# build the model\n",
    "model = BNr_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=20, batch_size=200,\n",
    "verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Recording loss and metric\n",
    "Below shows how to use Keras's callbacks for recording loss and metric. `keras.callbacks.History()` records events in to an object History(). Hading over the history object into `calback` option in `model.fit()` (or `model.compile()`) will return the output to `model.fit` as a dictionary. The dictionary has keys loss and metric for train and validation each. For our case here it would be: 'val_loss', 'val_acc', 'loss', 'acc'. A good use of such log is to monitor whether it's over fitting. When overfits, you will see the validation loss may go up at some point while train loss continues go down. Let's get rid of batch norm layers and run the model with higher running rate lr=0.01 and longer epoch (50) to see if it overfits (Answer: Yes it does, quite terribly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geena/anaconda3/envs/DLK1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1917: UserWarning: Expected no kwargs, you passed 1\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "3s - loss: 0.1951 - acc: 0.9381 - val_loss: 0.0788 - val_acc: 0.9751\n",
      "Epoch 2/50\n",
      "3s - loss: 0.0568 - acc: 0.9830 - val_loss: 0.0604 - val_acc: 0.9815\n",
      "Epoch 3/50\n",
      "3s - loss: 0.0497 - acc: 0.9853 - val_loss: 0.0594 - val_acc: 0.9818\n",
      "Epoch 4/50\n",
      "3s - loss: 0.0431 - acc: 0.9877 - val_loss: 0.0613 - val_acc: 0.9839\n",
      "Epoch 5/50\n",
      "3s - loss: 0.0408 - acc: 0.9883 - val_loss: 0.0529 - val_acc: 0.9854\n",
      "Epoch 6/50\n",
      "3s - loss: 0.0386 - acc: 0.9891 - val_loss: 0.0378 - val_acc: 0.9884\n",
      "Epoch 7/50\n",
      "3s - loss: 0.0384 - acc: 0.9892 - val_loss: 0.0464 - val_acc: 0.9856\n",
      "Epoch 8/50\n",
      "3s - loss: 0.0357 - acc: 0.9903 - val_loss: 0.0576 - val_acc: 0.9843\n",
      "Epoch 9/50\n",
      "3s - loss: 0.0435 - acc: 0.9891 - val_loss: 0.0511 - val_acc: 0.9878\n",
      "Epoch 10/50\n",
      "3s - loss: 0.0394 - acc: 0.9902 - val_loss: 0.0493 - val_acc: 0.9861\n",
      "Epoch 11/50\n",
      "3s - loss: 0.0302 - acc: 0.9920 - val_loss: 0.0490 - val_acc: 0.9872\n",
      "Epoch 12/50\n",
      "3s - loss: 0.0311 - acc: 0.9921 - val_loss: 0.0535 - val_acc: 0.9887\n",
      "Epoch 13/50\n",
      "3s - loss: 0.0410 - acc: 0.9904 - val_loss: 0.0477 - val_acc: 0.9884\n",
      "Epoch 14/50\n",
      "3s - loss: 0.0407 - acc: 0.9901 - val_loss: 0.0614 - val_acc: 0.9851\n",
      "Epoch 15/50\n",
      "3s - loss: 0.0330 - acc: 0.9919 - val_loss: 0.0557 - val_acc: 0.9875\n",
      "Epoch 16/50\n",
      "3s - loss: 0.0342 - acc: 0.9920 - val_loss: 0.0576 - val_acc: 0.9887\n",
      "Epoch 17/50\n",
      "3s - loss: 0.0353 - acc: 0.9917 - val_loss: 0.0519 - val_acc: 0.9876\n",
      "Epoch 18/50\n",
      "3s - loss: 0.0396 - acc: 0.9911 - val_loss: 0.0571 - val_acc: 0.9880\n",
      "Epoch 19/50\n",
      "3s - loss: 0.0329 - acc: 0.9921 - val_loss: 0.0871 - val_acc: 0.9838\n",
      "Epoch 20/50\n",
      "3s - loss: 0.0641 - acc: 0.9871 - val_loss: 0.0928 - val_acc: 0.9834\n",
      "Epoch 21/50\n",
      "3s - loss: 0.0659 - acc: 0.9869 - val_loss: 0.0595 - val_acc: 0.9886\n",
      "Epoch 22/50\n",
      "3s - loss: 0.0363 - acc: 0.9922 - val_loss: 0.0594 - val_acc: 0.9847\n",
      "Epoch 23/50\n",
      "3s - loss: 0.0420 - acc: 0.9914 - val_loss: 0.0866 - val_acc: 0.9843\n",
      "Epoch 24/50\n",
      "3s - loss: 0.0448 - acc: 0.9908 - val_loss: 0.0894 - val_acc: 0.9841\n",
      "Epoch 25/50\n",
      "3s - loss: 0.0392 - acc: 0.9921 - val_loss: 0.0694 - val_acc: 0.9877\n",
      "Epoch 26/50\n",
      "3s - loss: 0.0947 - acc: 0.9846 - val_loss: 0.1607 - val_acc: 0.9774\n",
      "Epoch 27/50\n",
      "3s - loss: 0.1525 - acc: 0.9758 - val_loss: 0.4193 - val_acc: 0.9449\n",
      "Epoch 28/50\n",
      "3s - loss: 1.4657 - acc: 0.7187 - val_loss: 2.0336 - val_acc: 0.3022\n",
      "Epoch 29/50\n",
      "3s - loss: 2.0393 - acc: 0.4327 - val_loss: 2.2240 - val_acc: 0.3661\n",
      "Epoch 30/50\n",
      "3s - loss: 3.0954 - acc: 0.2700 - val_loss: 2.3037 - val_acc: 0.1028\n",
      "Epoch 31/50\n",
      "3s - loss: 2.3576 - acc: 0.1122 - val_loss: 2.3018 - val_acc: 0.1010\n",
      "Epoch 32/50\n",
      "3s - loss: 2.3018 - acc: 0.1112 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 33/50\n",
      "3s - loss: 2.3018 - acc: 0.1119 - val_loss: 2.3016 - val_acc: 0.1135\n",
      "Epoch 34/50\n",
      "3s - loss: 2.3018 - acc: 0.1109 - val_loss: 2.3015 - val_acc: 0.1135\n",
      "Epoch 35/50\n",
      "3s - loss: 2.3018 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1135\n",
      "Epoch 36/50\n",
      "3s - loss: 2.3018 - acc: 0.1111 - val_loss: 2.3015 - val_acc: 0.1135\n",
      "Epoch 37/50\n",
      "3s - loss: 2.3018 - acc: 0.1124 - val_loss: 2.3017 - val_acc: 0.1135\n",
      "Epoch 38/50\n",
      "3s - loss: 2.3018 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1135\n",
      "Epoch 39/50\n",
      "2s - loss: 2.3019 - acc: 0.1110 - val_loss: 2.3014 - val_acc: 0.1135\n",
      "Epoch 40/50\n",
      "3s - loss: 2.3018 - acc: 0.1124 - val_loss: 2.3012 - val_acc: 0.1135\n",
      "Epoch 41/50\n",
      "3s - loss: 2.3019 - acc: 0.1103 - val_loss: 2.3014 - val_acc: 0.1135\n",
      "Epoch 42/50\n",
      "3s - loss: 2.3017 - acc: 0.1111 - val_loss: 2.3018 - val_acc: 0.1135\n",
      "Epoch 43/50\n",
      "3s - loss: 2.3016 - acc: 0.1121 - val_loss: 2.3017 - val_acc: 0.1135\n",
      "Epoch 44/50\n",
      "3s - loss: 2.3017 - acc: 0.1124 - val_loss: 2.3015 - val_acc: 0.1135\n",
      "Epoch 45/50\n",
      "3s - loss: 2.3017 - acc: 0.1098 - val_loss: 2.3015 - val_acc: 0.1135\n",
      "Epoch 46/50\n",
      "3s - loss: 2.3017 - acc: 0.1120 - val_loss: 2.3012 - val_acc: 0.1135\n",
      "Epoch 47/50\n",
      "3s - loss: 2.3017 - acc: 0.1124 - val_loss: 2.3014 - val_acc: 0.1135\n",
      "Epoch 48/50\n",
      "3s - loss: 2.3016 - acc: 0.1115 - val_loss: 2.3016 - val_acc: 0.1135\n",
      "Epoch 49/50\n",
      "3s - loss: 2.3018 - acc: 0.1118 - val_loss: 2.3011 - val_acc: 0.1135\n",
      "Epoch 50/50\n",
      "3s - loss: 2.3018 - acc: 0.1118 - val_loss: 2.3013 - val_acc: 0.1135\n",
      "157.122887134552  seconds\n",
      "CNN Error: 88.65%\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.callbacks import History\n",
    "import time\n",
    "\n",
    "K.set_image_dim_ordering( 'tf' )\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][width][height][channel]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype( 'float32' )\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype( 'float32' )\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "def model_overfit():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, 3, 3, input_shape=(28, 28, 1), border_mode='same'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(16, 3, 3, border_mode='valid'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='valid'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(64))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_classes))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('softmax'))\n",
    "    # Compile model\n",
    "    adam = Adam(lr=0.01)\n",
    "    history = History()\n",
    "    model.compile(loss= 'categorical_crossentropy' , optimizer= adam , metrics=[ 'accuracy' ], callbacks=history)\n",
    "    return model\n",
    "# build the model\n",
    "model = model_overfit()\n",
    "\n",
    "# Fit the model\n",
    "t0=time.time()\n",
    "log = model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=50, batch_size=200, verbose=2)\n",
    "t1=time.time()\n",
    "print(t1-t0,\" seconds\")\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f30c2373a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(log.history['acc'],'b') \n",
    "plt.plot(log.history['val_acc'],'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The case above is a quite drastic case because we deliberately made the model overfit by having high learning rate and longer epoch as well as removing removing batch norm and dropout layers. Note that the plot is of accuracy not loss. If you like, you may save the log.history (It's a dictionary) using pickle: https://wiki.python.org/moin/UsingPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1. Learning rate\n",
    "Without inserting batchnorm or dropout again, decrease learning rate and run for 50 epochs, plot the accuracy from train and validation. What is the highest learning rate that it doesn't overfit? What is the validation accuracy as a result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2. Dropout\n",
    "Now, add dropouts and run with the same hyperparameters (learning rate, epochs) you found from above. Time the model.fit() using `time.time`. \n",
    "1) Does it take longer training time by adding dropouts?\n",
    "2) For the same epoch, is your final validation accuracy better? If not better and you're sure it's not overfitting yet, try to increase either your learning rate or epoch, OR change your dropout rate(s). Record your optimum values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3. Batch Normalization\n",
    "Now, get rid of dropouts and add batch normalization layers. Choose learning rate between 0.01 and 0.001. Find the largest learning rate that still does not overfit but gives highest accuracy.\n",
    "Time model.fit() using `time.time`. \n",
    "Plot the 'acc' and 'val_acc'\n",
    "Compare the learning rate with those from Exercise 1 and 2. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
